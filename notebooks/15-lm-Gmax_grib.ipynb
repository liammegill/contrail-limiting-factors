{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9790ec45-d4b2-4cca-8f9d-d36f7a575e4f",
   "metadata": {},
   "source": [
    "# Calculate histograms of the maximum mixing line slope\n",
    "\n",
    "_Dataset:_ Supplementary data for Megill and Grewe (2024): \"Investigating the limiting aircraft design-dependent and environmental factors of persistent contrail formation\".\n",
    "\n",
    "_Authors:_\n",
    "\n",
    "- Liam Megill (1, 2), https://orcid.org/0000-0002-4199-6962   \n",
    "- Volker Grewe (1, 2), https://orcid.org/0000-0002-8012-6783  \n",
    "\n",
    "_Affiliation (1)_: Deutsches Zentrum für Luft- und Raumfahrt (DLR), Institut für Physik der Atmosphäre, Oberpfaffenhofen, Germany\n",
    "\n",
    "_Affiliation (2)_: Delft University of Technology (TU Delft), Faculty of Aerospace Engineering, Section Aircraft Noise and Climate Effects (ANCE), Delft, The Netherlands\n",
    "\n",
    "_Corresponding author_: Liam Megill, liam.megill@dlr.de\n",
    "\n",
    "_doi_: https://doi.org/10.5194/egusphere-2024-3398\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Summary\n",
    "This notebook calculates histograms of the maximum mixing line slope $G_{max}$ using ERA5 data between `start_date` and `end_date`. This mixing line slope is the steepest slope that an aircraft can have for any given ambient conditions before a persistent contrail begins to form (see the second figure created in `11-lm-supporting_graphs.ipynb`). By cumulatively summing the histograms, the potential persistent contrail formation can be obtained as a function of the mixing line slope. This is done in `17-lm-analyse_Gmax.ipynb`. One month takes around 40 minutes to calculate on DKRZ Levante and requires approximately 15 GB of RAM.\n",
    "\n",
    "### Inputs\n",
    "- ERA5 GRIB data: If not performing the study on DKRZ Levante, the ERA5 GRIB data needs to be saved locally and `dir_path` updated. We recommend placing the ERA5 files in `data/raw/`. Ensure Ensure that the file naming matches that of `t_file_path` and `r_file_path`.\n",
    "\n",
    "### Outputs\n",
    "- `data/processed/ppcf/ppcfhist_1M_{YYYY-MM}_ERA5_GRIB_{cor_savename_ext}.nc`: Histogram for a given combination of year, month and RHi enhancement (\"correction\"). \n",
    "\n",
    "---\n",
    "\n",
    "### Copyright\n",
    "\n",
    "Copyright © 2024 Liam Megill\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe31f0-d12b-492d-a876-0f0abdad3524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import datetime\n",
    "import cf2cdm\n",
    "import warnings\n",
    "\n",
    "# define directories\n",
    "project_dir = \"\"  # set top-level directory path\n",
    "processed_data_dir = project_dir + \"data/processed/ppcf/\"\n",
    "\n",
    "# dates\n",
    "start_date = datetime.date(2010, 1, 1)\n",
    "end_date = datetime.date(2010, 12, 1)\n",
    "\n",
    "# other options\n",
    "test_savename_ext = False  # this adds \"test\" to the savename\n",
    "rhi_cor = 1.0  # correction to RHi\n",
    "cor_savename_ext = \"uncor\"  # this gets added to the end of the savename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a76091-551f-4719-94da-553214febf85",
   "metadata": {},
   "source": [
    "The first step is to format the dates that will be analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06073a5-2b51-444e-bdcd-3ef0984c2eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper import find_month_boundaries\n",
    "\n",
    "# create dates\n",
    "date_arr = [start_date + datetime.timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n",
    "formatted_date_arr = [date.strftime(\"%Y-%m-%d\") for date in date_arr]\n",
    "date_dt64 = np.array(date_arr, dtype=\"datetime64[ns]\")\n",
    "\n",
    "# find month start and end dates\n",
    "month_dates = find_month_boundaries(start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f955457-658a-4f11-95ca-d646026bc239",
   "metadata": {},
   "source": [
    "The next step is to calculate the histograms. We start by defining the histogram, which in the linked study has a `bin_size` of 0.2 with `bin_limits` of [0.0, 4.6]. We then loop through each month, ensuring that only full months are run. For each day in the month, we load the ERA5 data from file, select the relevant pressure levels and calculate the histogram using the `calc_hist_arr` function. We save the results on a monthly basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba23fda-b2d1-4040-a6e2-be446f5d5103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper import calc_hist_arr\n",
    "\n",
    "# define histogram\n",
    "bin_size = 0.2\n",
    "bin_limits = [0., 4.6]\n",
    "bin_edges = np.arange(bin_limits[0], bin_limits[1] + bin_size, bin_size)\n",
    "bin_centres = bin_edges[:-1] + 0.5 * np.diff(bin_edges)\n",
    "\n",
    "# do loop over months\n",
    "for i_mon, (mon_start, mon_end) in enumerate(month_dates):\n",
    "    date_idxs = np.where((np.array(date_arr) >= mon_start) & (np.array(date_arr) <= mon_end))[0]  # the np.where function produces an extra level\n",
    "    \n",
    "    # check if all days within a month are included. If not, do not run the month (prevents monthly data that is not complete!)\n",
    "    if len(date_idxs) == mon_end.day-mon_start.day+1:  \n",
    "        print(mon_start)\n",
    "    \n",
    "        # do loop over days in month\n",
    "        for idx in date_idxs: \n",
    "            date = formatted_date_arr[idx]\n",
    "            \n",
    "            # load GRIB files\n",
    "            dir_path = \"/pool/data/ERA5/E5/pl/an/1H/\"\n",
    "            t_file_path = \"130/E5pl00_1H_{}_130.grb\".format(date)  # temperature file (130)\n",
    "            r_file_path = \"157/E5pl00_1H_{}_157.grb\".format(date)  # relative humidity file (157)\n",
    "            dsg_t = xr.open_dataset(dir_path+t_file_path, engine=\"cfgrib\", backend_kwargs={\"indexpath\":None})\n",
    "            dsg_r = xr.open_dataset(dir_path+r_file_path, engine=\"cfgrib\", backend_kwargs={\"indexpath\":None})\n",
    "            dsg = xr.merge([dsg_t, dsg_r])\n",
    "            with warnings.catch_warnings():  # ignoring UserWarning from cf2cdm when converting coordinate time -> time\n",
    "                warnings.simplefilter('ignore')\n",
    "                dsg = cf2cdm.translate_coords(dsg, cf2cdm.ECMWF)  # convert to ECMWF coordinates\n",
    "            dsg = dsg.isel(level=[18, 19, 20, 21, 22, 23, 24])  # selecting only the levels that are interesting\n",
    "\n",
    "            # calculate daily histogram\n",
    "            res = calc_hist_arr(dsg, bin_edges, bin_centres, rhi_cor)\n",
    "            \n",
    "            # if first day of the month, initialise full array\n",
    "            if idx == date_idxs[0]:\n",
    "                hist_arr = res\n",
    "            else:\n",
    "                hist_arr += res\n",
    "        \n",
    "        # define and save monthly histogram as dataset\n",
    "        ds_hist = xr.Dataset({\"tot_hist\": ([\"level\", \"bin_centre\"], hist_arr[:, 0, :]),\n",
    "                  \"xtropN_hist\": ([\"level\", \"bin_centre\"], hist_arr[:, 1, :]),\n",
    "                  \"trop_hist\": ([\"level\", \"bin_centre\"], hist_arr[:, 2, :]),\n",
    "                  \"xtropS_hist\": ([\"level\", \"bin_centre\"], hist_arr[:, 3, :])},\n",
    "                 coords={\"level\": dsg.level.values, \"bin_centre\": bin_centres})\n",
    "        ds_hist.tot_hist.attrs.update({\"units\": \"-\", \"long_name\": \"tot_hist\",\n",
    "                                       \"description\": \"Non-density histogram of all G_max\"})\n",
    "        ds_hist.xtropN_hist.attrs.update({\"units\": \"-\", \"long_name\": \"xtropN_hist\",\n",
    "                                          \"description\": \"Non-density histogram of G_max in the Northern extratropics (>30deg lat)\"})\n",
    "        ds_hist.trop_hist.attrs.update({\"units\": \"-\", \"long_name\": \"trop_hist\",\n",
    "                                        \"description\": \"Non-density histogram of G_max in the tropics (-30deg <= lat <= 30deg)\"})\n",
    "        ds_hist.xtropS_hist.attrs.update({\"units\": \"-\", \"long_name\": \"xtropS_hist\",\n",
    "                                          \"description\": \"Non-density histogram of G_max in the Southern extratropics (<-30deg lat)\"})\n",
    "        ds_hist.attrs.update({\"author\": \"Liam Megill\",\n",
    "                              \"institution\": \"Deutsches Zentrum für Luft- und Raumfahrt, Institute of Atmospheric Physics\",\n",
    "                              \"description\": \"Monthly non-density histogram of G_max, calculated using ERA5 GRIB data stored on DKRZ Levante\",\n",
    "                              \"bin_definition\": f\"[{bin_limits[0]}:{bin_limits[1]}] with bin size {bin_size} [Pa/K]\",\n",
    "                              \"num_vals\": f\"{len(date_idxs) * 24 * len(dsg.latitude)}\",\n",
    "                              \"timespan\": mon_start.strftime(\"%b %Y\"),\n",
    "                              \"created\": \"{} CET\".format(datetime.datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")),\n",
    "                              \"corrections\": cor_savename_ext})\n",
    "        savename = f\"{'test_' if test_savename_ext else ''}ppcfhist_1M_{mon_start.strftime('%Y-%m')}_ERA5_GRIB_{cor_savename_ext}.nc\"\n",
    "        ds_hist.to_netcdf(processed_data_dir+savename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d2fe5-a039-491f-ab5c-dd5b66676080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1 Python 3 (based on the module python3/2023.01)",
   "language": "python",
   "name": "python3_2023_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
